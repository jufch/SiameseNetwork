{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_model import ModelTrainer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import of optimizer\n",
    "from keras.optimizers import Adam, RMSprop, AdamW\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 1)\n",
    "num_classes = 3\n",
    "\n",
    "#Optimizer\n",
    "optimizer_Adam_0001 = Adam(learning_rate=0.0001)\n",
    "optimizer_Adam_001 = Adam(learning_rate=0.001)\n",
    "\n",
    "optimizer_AdamW_01 = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "optimizer_AdamW_001 = AdamW(learning_rate=0.0001, weight_decay=0.01)\n",
    "\n",
    "optimizer_RMSprop_001 = RMSprop(learning_rate=0.001)\n",
    "optimizer_RMSprop_01 = RMSprop(learning_rate=0.01)\n",
    "\n",
    "# Define optimizers\n",
    "optimizers = {\n",
    "    'Adam_0001': Adam(learning_rate=0.0001),\n",
    "    'Adam_001': Adam(learning_rate=0.001),\n",
    "    'AdamW_01': AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    'AdamW_001': AdamW(learning_rate=0.0001, weight_decay=0.01),\n",
    "    'RMSprop_001': RMSprop(learning_rate=0.001),\n",
    "    'RMSprop_01': RMSprop(learning_rate=0.01)\n",
    "}\n",
    "\n",
    "# Loss function\n",
    "loss_sparse = 'sparse_categorical_crossentropy'\n",
    "loss_focal = 'focal_loss'\n",
    "\n",
    "# Batch size\n",
    "batch_size_32 = 32\n",
    "batch_size_64 = 64\n",
    "batch_size_128 = 128\n",
    "\n",
    "# Define loss functions\n",
    "loss_functions = ['sparse_categorical_crossentropy', 'focal_loss']\n",
    "\n",
    "# Define batch sizes\n",
    "batch_sizes = [32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/train.csv\"\n",
    "validation_path = os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/validation.csv\"\n",
    "test_path = os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/test.csv\"\n",
    "image_dir = os.getcwd() + \"/OpenSARShip/Categories/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNNModel object\n",
    "def simple_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment1 = ModelTrainer(model, os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/train.csv\", \n",
    "#                           os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/validation.csv\", \n",
    "#                           os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/test.csv\", \n",
    "#                           os.getcwd() + \"/OpenSARShip/Categories/\", seed=42)\n",
    "\n",
    "# experiment1.load_data()\n",
    "# experiment1.compile_model(optimizer_Adam_0001, loss_sparse, ['accuracy'])\n",
    "# experiment1.train_model(batch_size_32, epochs=100, verbose=1, early_stopping=True, patience=20, model_save_name=\"simu_1.h5\")\n",
    "# experiment1.evaluate_model()\n",
    "# experiment1.plot_training_history()\n",
    "# experiment1.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform experiments\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    for loss_function in loss_functions:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Create experiment instance\n",
    "            experiment = ModelTrainer(model, train_path, validation_path, test_path, image_dir, seed=42)\n",
    "            experiment.load_data()\n",
    "            experiment.compile_model(optimizer, loss_function, ['accuracy'])\n",
    "            experiment.train_model(batch_size, epochs=100, verbose=1, early_stopping=True, patience=20, model_save_name=f\"model_{optimizer_name}_{loss_function}_{batch_size}.h5\")\n",
    "            experiment.evaluate_model()\n",
    "            experiment.plot_training_history()\n",
    "            experiment.plot_confusion_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
