{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_model import ModelTrainer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import of optimizer\n",
    "from keras.optimizers import Adam, RMSprop, AdamW\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import of focal loss\n",
    "# from tf.keras.losses import CategoricalFocalCrossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 1)\n",
    "num_classes = 3\n",
    "\n",
    "#Optimizer\n",
    "optimizer_Adam_0001 = Adam(learning_rate=0.0001)\n",
    "optimizer_Adam_001 = Adam(learning_rate=0.001)\n",
    "\n",
    "optimizer_AdamW_01 = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "optimizer_AdamW_001 = AdamW(learning_rate=0.0001, weight_decay=0.01)\n",
    "\n",
    "optimizer_RMSprop_001 = RMSprop(learning_rate=0.001)\n",
    "optimizer_RMSprop_01 = RMSprop(learning_rate=0.01)\n",
    "\n",
    "# Define optimizers\n",
    "optimizers = {\n",
    "    'Adam_0001': Adam(learning_rate=0.0001),\n",
    "    'Adam_001': Adam(learning_rate=0.001),\n",
    "    'AdamW_01': AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    'AdamW_001': AdamW(learning_rate=0.0001, weight_decay=0.01),\n",
    "    'RMSprop_001': RMSprop(learning_rate=0.001),\n",
    "    'RMSprop_01': RMSprop(learning_rate=0.01)\n",
    "}\n",
    "\n",
    "# Loss function\n",
    "loss_sparse = 'sparse_categorical_crossentropy'\n",
    "# loss_focal = 'focal_loss'\n",
    "\n",
    "# Batch size\n",
    "batch_size_32 = 32\n",
    "batch_size_64 = 64\n",
    "batch_size_128 = 128\n",
    "\n",
    "# Define loss functions\n",
    "loss_functions = ['sparse_categorical_crossentropy', 'focal_loss']\n",
    "\n",
    "# Define batch sizes\n",
    "batch_sizes = [32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.getcwd() + \"/Split_Tanker_Bulk_Container_frugal_vv/train.csv\"\n",
    "validation_path = os.getcwd() + \"/Split_Tanker_Bulk_Container_frugal_vv/validation.csv\"\n",
    "test_path = os.getcwd() + \"/Split_Tanker_Bulk_Container_frugal_vv/test.csv\"\n",
    "\n",
    "image_dir = '../OpenSARShip/Categories/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNNModel object\n",
    "def simple_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment1 = ModelTrainer(model, os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/train.csv\", \n",
    "#                           os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/validation.csv\", \n",
    "#                           os.getcwd() + \"/SiameseNetwork/Split_Tanker_Bulk_Container_frugal_vv/test.csv\", \n",
    "#                           os.getcwd() + \"/OpenSARShip/Categories/\", seed=42)\n",
    "\n",
    "# experiment1.load_data()\n",
    "# experiment1.compile_model(optimizer_Adam_0001, loss_sparse, ['accuracy'])\n",
    "# experiment1.train_model(batch_size_32, epochs=100, verbose=1, early_stopping=True, patience=20, model_save_name=\"simu_1.h5\")\n",
    "# experiment1.evaluate_model()\n",
    "# experiment1.plot_training_history()\n",
    "# experiment1.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting with parameters: Optimizer=Adam_0001, Loss=sparse_categorical_crossentropy, Batch Size=32, Epochs=10\n",
      "Found 629 validated image filenames belonging to 3 classes.\n",
      "Found 211 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Julia Fouchier\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 1 invalid image filename(s) in x_col=\"file_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Julia Fouchier\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 1 invalid image filename(s) in x_col=\"file_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 1.0370 - accuracy: 0.5394\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61458, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Julia Fouchier\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 22s 879ms/step - loss: 1.0370 - accuracy: 0.5394 - val_loss: 0.9218 - val_accuracy: 0.6146\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.9365 - accuracy: 0.5528\n",
      "Epoch 2: val_accuracy improved from 0.61458 to 0.65104, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n",
      "19/19 [==============================] - 18s 738ms/step - loss: 0.9365 - accuracy: 0.5528 - val_loss: 0.7967 - val_accuracy: 0.6510\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.6097\n",
      "Epoch 3: val_accuracy improved from 0.65104 to 0.69792, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n",
      "19/19 [==============================] - 17s 710ms/step - loss: 0.8362 - accuracy: 0.6097 - val_loss: 0.7282 - val_accuracy: 0.6979\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7922 - accuracy: 0.6678\n",
      "Epoch 4: val_accuracy did not improve from 0.69792\n",
      "19/19 [==============================] - 16s 648ms/step - loss: 0.7922 - accuracy: 0.6678 - val_loss: 0.7423 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.6633\n",
      "Epoch 5: val_accuracy improved from 0.69792 to 0.70833, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n",
      "19/19 [==============================] - 17s 702ms/step - loss: 0.7912 - accuracy: 0.6633 - val_loss: 0.7056 - val_accuracy: 0.7083\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7517 - accuracy: 0.6734\n",
      "Epoch 6: val_accuracy did not improve from 0.70833\n",
      "19/19 [==============================] - 16s 654ms/step - loss: 0.7517 - accuracy: 0.6734 - val_loss: 0.7138 - val_accuracy: 0.6979\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.6683\n",
      "Epoch 7: val_accuracy improved from 0.70833 to 0.73958, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n",
      "19/19 [==============================] - 18s 760ms/step - loss: 0.7550 - accuracy: 0.6683 - val_loss: 0.6457 - val_accuracy: 0.7396\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.7236\n",
      "Epoch 8: val_accuracy improved from 0.73958 to 0.76042, saving model to model_Adam_0001_sparse_categorical_crossentropy_32.h5\n",
      "19/19 [==============================] - 20s 869ms/step - loss: 0.7017 - accuracy: 0.7236 - val_loss: 0.6274 - val_accuracy: 0.7604\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.7353\n",
      "Epoch 9: val_accuracy did not improve from 0.76042\n",
      "19/19 [==============================] - 17s 676ms/step - loss: 0.6481 - accuracy: 0.7353 - val_loss: 0.6017 - val_accuracy: 0.7396\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.7454\n",
      "Epoch 10: val_accuracy did not improve from 0.76042\n",
      "19/19 [==============================] - 17s 679ms/step - loss: 0.6128 - accuracy: 0.7454 - val_loss: 0.6940 - val_accuracy: 0.6927\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't decrement id ref count (unable to extend file properly)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m experiment\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     11\u001b[0m experiment\u001b[38;5;241m.\u001b[39mcompile_model(optimizer, loss_function, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m experiment\u001b[38;5;241m.\u001b[39mevaluate_model(batch_size)\n\u001b[0;32m     14\u001b[0m experiment\u001b[38;5;241m.\u001b[39mplot_training_history(epochs)\n",
      "File \u001b[1;32mc:\\Users\\Julia Fouchier\\Projet_syteme_3A\\SiameseNetwork\\CNN_model.py:94\u001b[0m, in \u001b[0;36mModelTrainer.train_model\u001b[1;34m(self, batch_size, epochs, model_save_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m     best_val_accuracy \u001b[38;5;241m=\u001b[39m final_val_accuracy\n\u001b[0;32m     93\u001b[0m     best_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_save_name\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew best model with validation accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Julia Fouchier\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Julia Fouchier\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\h5py\\_hl\\files.py:581\u001b[0m, in \u001b[0;36mFile.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \n\u001b[0;32m    578\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close_open_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_LOCAL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    584\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:355\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't decrement id ref count (unable to extend file properly)"
     ]
    }
   ],
   "source": [
    "# Perform experiments\n",
    "epochs = 10\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    for loss_function in loss_functions:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Experimenting with parameters: Optimizer={optimizer_name}, Loss={loss_function}, Batch Size={batch_size}, Epochs={epochs}\")\n",
    "            # Create experiment instance\n",
    "            model_save_name = f\"model_{optimizer_name}_{loss_function}_{batch_size}.h5\"\n",
    "            experiment = ModelTrainer(model, train_path, validation_path, test_path, image_dir, seed=42)\n",
    "            experiment.load_data()\n",
    "            experiment.compile_model(optimizer, loss_function, ['accuracy'])\n",
    "            experiment.train_model(batch_size, epochs, model_save_name)\n",
    "            experiment.evaluate_model(batch_size)\n",
    "            experiment.plot_training_history(epochs)\n",
    "            experiment.plot_confusion_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
