{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Siamese_model import SiameseTrainer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from Siamese_model import SiameseTrainer\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your custom binary cross-entropy loss function as before\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    loss = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "# Step 1: Load the pretrained model\n",
    "model_path = os.getcwd() + '/Siam_model_mstar.h5'\n",
    "# pretrained_model = load_model(model_path, custom_objects={'custom_binary_crossentropy': custom_binary_crossentropy})\n",
    "\n",
    "# # Freeze some layers\n",
    "# for layer in pretrained_model.layers[:-3]:\n",
    "#     layer.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 7 layers, found 2 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m new_model((\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Julia\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Julia\\anaconda3\\envs\\tensorflow_new\\lib\\site-packages\\keras\\src\\saving\\legacy\\hdf5_format.py:819\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    817\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m     )\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 7 layers, found 2 saved layers."
     ]
    }
   ],
   "source": [
    "def new_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.001)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(256, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = new_model((96, 96, 1), 3)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, target_size=(96, 96)):\n",
    "    img = load_img(path, target_size=target_size, color_mode='grayscale', interpolation='lanczos')\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img /= 255.0  # Normalize to [0,1]\n",
    "    return img\n",
    "\n",
    "def load_pairs_and_labels(csv_path, image_base_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pair_images = []\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        img1_path = os.path.join(image_base_path, row['image_1'])\n",
    "        img2_path = os.path.join(image_base_path, row['image_2'])\n",
    "        \n",
    "        img1 = load_and_preprocess_image(img1_path)\n",
    "        img2 = load_and_preprocess_image(img2_path)\n",
    "        \n",
    "        pair_images.append(np.concatenate([img1, img2], axis=0))\n",
    "    \n",
    "    # Convert list of pairs to a numpy array\n",
    "    pairs = np.array(pair_images)\n",
    "    \n",
    "    # Since each image pair is concatenated along axis=0, we reshape to ensure\n",
    "    # the final structure is compatible with what train_model expects\n",
    "    pairs = pairs.reshape(-1, 2, *pairs.shape[2:])\n",
    "    \n",
    "    return pairs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 1)\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for quater dataset\n",
    "image_base_path = '../OpenSARShip/Categories/'\n",
    "\n",
    "train_csv_path = os.getcwd() + \"/Split_quater_siamese/train_pairs.csv\"\n",
    "val_csv_path = os.getcwd() + \"/Split_quater_siamese/val_pairs.csv\"\n",
    "test_csv_path = os.getcwd() + \"/Split_quater_siamese/test_pairs.csv\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "print(\"Loading and preprocessing the data train\")\n",
    "train_pairs, train_labels = load_pairs_and_labels(train_csv_path, image_base_path)\n",
    "print(\"Loading and preprocessing the data val\")\n",
    "val_pairs, val_labels = load_pairs_and_labels(val_csv_path, image_base_path)\n",
    "print(\"Loading and preprocessing the data test\")\n",
    "test_pairs, test_labels = load_pairs_and_labels(test_csv_path, image_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `simple_cnn` and `SiameseTrainer` are defined as before\n",
    "print(\"Creating the SiameseTrainer\")\n",
    "siamese_network = SiameseTrainer(pretrained_model, input_shape=(96, 96, 1), num_classes=num_classes)\n",
    "# siamese_network = SiameseTrainer(base_model_func=new_model, input_shape=(96, 96, 1), num_classes=num_classes)\n",
    "\n",
    "# Set the optimizer\n",
    "initial_learning_rate = 0.0001\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=1000,\n",
    "#     decay_rate=0.96,\n",
    "#     staircase=True)\n",
    "\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "print(\"Compiling the model\")\n",
    "# siamese_network.compile_model(optimizer='adam', loss='binary_crossentropy')\n",
    "siamese_network.compile_model(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode = 'min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode = 'min')\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model\")\n",
    "# siamese_network.train_model(train_pairs, train_labels, val_pairs, val_labels, epochs=10, batch_size=32)\n",
    "siamese_network.train_model(train_pairs, train_labels, val_pairs, val_labels, epochs=30, batch_size=128, model_save_name='Siam_transfert_mstar.h5',\n",
    "                            callbacks=[reduce_lr])\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "print(\"Plotting the training history\")\n",
    "siamese_network.plot_training()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model\")\n",
    "siamese_network.evaluate_model(test_pairs, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
